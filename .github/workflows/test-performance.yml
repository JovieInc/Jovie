name: Test Performance Monitoring

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      run_flaky_detection:
        description: 'Run flaky test detection (runs tests multiple times)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read
  actions: read
  issues: write
  pull-requests: write

concurrency:
  group: test-perf-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  performance-guard:
    name: Performance Guard
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      guard_status: ${{ steps.guard-results.outputs.status }}
      guard_duration: ${{ steps.guard-results.outputs.duration_ms }}

    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with: { fetch-depth: 1 }

      - name: Setup Node.js and pnpm
        uses: ./.github/actions/setup-node-pnpm

      - name: Run Performance Guard
        id: performance-guard
        run: |
          set -euo pipefail
          echo "Running performance guard..."
          START_TIME=$(date +%s%N)

          if pnpm test:guard 2>&1 | tee guard-output.txt; then
            echo "guard_result=success" >> $GITHUB_OUTPUT
          else
            echo "guard_result=failure" >> $GITHUB_OUTPUT
          fi

          END_TIME=$(date +%s%N)
          DURATION_MS=$(( (END_TIME - START_TIME) / 1000000 ))
          echo "duration_ms=$DURATION_MS" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Parse Guard Results
        id: guard-results
        run: |
          set -euo pipefail

          DURATION="${{ steps.performance-guard.outputs.duration_ms }}"
          RESULT="${{ steps.performance-guard.outputs.guard_result }}"

          echo "status=$RESULT" >> $GITHUB_OUTPUT
          echo "duration_ms=$DURATION" >> $GITHUB_OUTPUT

          if [ "$RESULT" == "failure" ]; then
            echo "::warning::Performance guard failed - tests may be too slow"
          fi

      - name: Performance Guard Results
        if: steps.performance-guard.outputs.guard_result == 'failure'
        run: |
          echo "‚ùå Performance guard failed - tests are too slow!"
          echo "Run 'pnpm test:profile' locally for detailed analysis"
          exit 1

      - name: Success Message
        if: steps.performance-guard.outputs.guard_result == 'success'
        run: |
          echo "‚úÖ All performance thresholds met!"
          echo "üöÄ Test suite is optimized for fast feedback"
          echo "Duration: ${{ steps.performance-guard.outputs.duration_ms }}ms"

  fast-tests:
    name: Fast Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      test_status: ${{ steps.test-results.outputs.status }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      duration_ms: ${{ steps.test-results.outputs.duration_ms }}
      pass_rate: ${{ steps.test-results.outputs.pass_rate }}

    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with: { fetch-depth: 1 }

      - name: Setup Node.js and pnpm
        uses: ./.github/actions/setup-node-pnpm

      - name: Run Fast Test Suite
        id: run-tests
        run: |
          set -euo pipefail
          START_TIME=$(date +%s%N)

          # Run tests with JSON reporter for parsing
          if pnpm test:fast --reporter=json --reporter=default 2>&1 | tee test-output.txt; then
            echo "test_result=success" >> $GITHUB_OUTPUT
          else
            echo "test_result=failure" >> $GITHUB_OUTPUT
          fi

          END_TIME=$(date +%s%N)
          DURATION_MS=$(( (END_TIME - START_TIME) / 1000000 ))
          echo "duration_ms=$DURATION_MS" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Parse Test Results
        id: test-results
        run: |
          set -euo pipefail

          DURATION="${{ steps.run-tests.outputs.duration_ms }}"
          RESULT="${{ steps.run-tests.outputs.test_result }}"

          # Parse test counts from output
          if [ -f test-output.txt ]; then
            # Extract test counts from Vitest output
            TOTAL=$(grep -oP 'Tests\s+\d+' test-output.txt | grep -oP '\d+' | tail -1 || echo "0")
            PASSED=$(grep -oP '\d+\s+passed' test-output.txt | grep -oP '^\d+' | tail -1 || echo "0")
            FAILED=$(grep -oP '\d+\s+failed' test-output.txt | grep -oP '^\d+' | tail -1 || echo "0")

            # Fallback: try alternative patterns if above didn't work
            if [ "$TOTAL" == "0" ]; then
              TOTAL=$(grep -c 'PASS\|FAIL' test-output.txt || echo "0")
              PASSED=$(grep -c 'PASS' test-output.txt || echo "0")
              FAILED=$(grep -c 'FAIL' test-output.txt || echo "0")
            fi
          else
            TOTAL=0
            PASSED=0
            FAILED=0
          fi

          # Calculate pass rate
          if [ "$TOTAL" -gt 0 ]; then
            PASS_RATE=$(awk "BEGIN {printf \"%.2f\", ($PASSED / $TOTAL) * 100}")
          else
            PASS_RATE="0.00"
          fi

          echo "status=$RESULT" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL" >> $GITHUB_OUTPUT
          echo "passed_tests=$PASSED" >> $GITHUB_OUTPUT
          echo "failed_tests=$FAILED" >> $GITHUB_OUTPUT
          echo "duration_ms=$DURATION" >> $GITHUB_OUTPUT
          echo "pass_rate=$PASS_RATE" >> $GITHUB_OUTPUT

          echo "üìä Test Results: $PASSED/$TOTAL passed ($PASS_RATE%)"
          echo "‚è±Ô∏è Duration: ${DURATION}ms"

      - name: Upload Test Output
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08 # v4.6.0
        if: always()
        with:
          name: fast-test-output
          path: test-output.txt
          retention-days: 7

      - name: Fail if tests failed
        if: steps.run-tests.outputs.test_result == 'failure'
        run: |
          echo "‚ùå Fast test suite failed"
          exit 1

  flaky-test-detection:
    name: Flaky Test Detection
    runs-on: ubuntu-latest
    timeout-minutes: 20
    # Only run on push to main, manual dispatch with flag, or scheduled
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_flaky_detection == 'true')
    outputs:
      flaky_count: ${{ steps.flaky-results.outputs.flaky_count }}
      flaky_tests: ${{ steps.flaky-results.outputs.flaky_tests }}
      detection_status: ${{ steps.flaky-results.outputs.status }}
      total_runs: ${{ steps.flaky-results.outputs.total_runs }}

    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with: { fetch-depth: 1 }

      - name: Setup Node.js and pnpm
        uses: ./.github/actions/setup-node-pnpm

      - name: Detect Flaky Tests
        id: detect-flaky
        run: |
          set -euo pipefail
          echo "üîç Running flaky test detection (5 iterations)..."

          # Run flaky detection and capture output
          if pnpm test:flaky "" 5 2>&1 | tee flaky-output.txt; then
            echo "detection_result=success" >> $GITHUB_OUTPUT
          else
            echo "detection_result=warning" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Parse Flaky Results
        id: flaky-results
        run: |
          set -euo pipefail

          RESULT="${{ steps.detect-flaky.outputs.detection_result }}"
          FLAKY_COUNT=0
          FLAKY_TESTS=""

          # Check for quarantine file updates
          QUARANTINE_FILE="tests/quarantine/flaky-tests.json"
          if [ -f "$QUARANTINE_FILE" ]; then
            FLAKY_COUNT=$(jq -r 'length' "$QUARANTINE_FILE" 2>/dev/null || echo "0")
            FLAKY_TESTS=$(jq -r 'keys | join(", ")' "$QUARANTINE_FILE" 2>/dev/null || echo "")
          fi

          # Also check flaky-output.txt for detected flaky tests
          if [ -f flaky-output.txt ]; then
            DETECTED=$(grep -c 'FLAKY:' flaky-output.txt || echo "0")
            if [ "$DETECTED" -gt 0 ]; then
              FLAKY_COUNT=$DETECTED
              FLAKY_TESTS=$(grep 'FLAKY:' flaky-output.txt | sed 's/FLAKY: //' | tr '\n' ', ' | sed 's/,$//')
            fi
          fi

          echo "status=$RESULT" >> $GITHUB_OUTPUT
          echo "flaky_count=$FLAKY_COUNT" >> $GITHUB_OUTPUT
          echo "flaky_tests=$FLAKY_TESTS" >> $GITHUB_OUTPUT
          echo "total_runs=5" >> $GITHUB_OUTPUT

          if [ "$FLAKY_COUNT" -gt 0 ]; then
            echo "‚ö†Ô∏è Detected $FLAKY_COUNT flaky test(s): $FLAKY_TESTS"
          else
            echo "‚úÖ No flaky tests detected"
          fi

      - name: Upload Quarantine Results
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08 # v4.6.0
        if: always()
        with:
          name: flaky-test-results
          path: |
            tests/quarantine/
            flaky-output.txt
          retention-days: 30

  performance-report:
    name: Performance Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    # Only run on push to main (not PRs or merge queue)
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    outputs:
      total_duration_ms: ${{ steps.parse-metrics.outputs.total_duration_ms }}
      setup_time_ms: ${{ steps.parse-metrics.outputs.setup_time_ms }}
      p95_ms: ${{ steps.parse-metrics.outputs.p95_ms }}
      slow_test_count: ${{ steps.parse-metrics.outputs.slow_test_count }}

    steps:
      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with: { fetch-depth: 1 }

      - name: Setup Node.js and pnpm
        uses: ./.github/actions/setup-node-pnpm

      - name: Generate Performance Report
        id: generate-report
        run: |
          set -euo pipefail
          echo "üìä Generating performance report..."

          if pnpm test:profile 2>&1 | tee profile-output.txt; then
            echo "profile_result=success" >> $GITHUB_OUTPUT
          else
            echo "profile_result=failure" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Parse Performance Metrics
        id: parse-metrics
        run: |
          set -euo pipefail

          BASELINE_FILE="test-performance-baseline.json"

          if [ -f "$BASELINE_FILE" ]; then
            TOTAL_DURATION=$(jq -r '.metrics.totalDuration // 0' "$BASELINE_FILE")
            SETUP_TIME=$(jq -r '.metrics.setupTime // 0' "$BASELINE_FILE")
            P95=$(jq -r '.metrics.performanceStats.p95 // 0' "$BASELINE_FILE")
            SLOW_COUNT=$(jq -r '.metrics.slowTests | length // 0' "$BASELINE_FILE")

            echo "total_duration_ms=$TOTAL_DURATION" >> $GITHUB_OUTPUT
            echo "setup_time_ms=$SETUP_TIME" >> $GITHUB_OUTPUT
            echo "p95_ms=$P95" >> $GITHUB_OUTPUT
            echo "slow_test_count=$SLOW_COUNT" >> $GITHUB_OUTPUT

            # Create metrics summary
            echo "üìà Performance Metrics:"
            echo "  Total Duration: ${TOTAL_DURATION}ms ($(awk "BEGIN {printf \"%.1f\", $TOTAL_DURATION/1000}")s)"
            echo "  Setup Time: ${SETUP_TIME}ms"
            echo "  P95: ${P95}ms"
            echo "  Slow Tests: $SLOW_COUNT"
          else
            echo "total_duration_ms=0" >> $GITHUB_OUTPUT
            echo "setup_time_ms=0" >> $GITHUB_OUTPUT
            echo "p95_ms=0" >> $GITHUB_OUTPUT
            echo "slow_test_count=0" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No baseline file found"
          fi

      - name: Upload Performance Baseline
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08 # v4.6.0
        with:
          name: performance-baseline
          path: |
            test-performance-baseline.json
            profile-output.txt
          retention-days: 90

  # Reliability metrics summary job - aggregates all metrics
  reliability-metrics:
    name: Reliability Metrics Summary
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [performance-guard, fast-tests, flaky-test-detection, performance-report]
    if: always() && github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Generate Metrics Summary
        id: summary
        env:
          GUARD_STATUS: ${{ needs.performance-guard.outputs.guard_status }}
          GUARD_DURATION: ${{ needs.performance-guard.outputs.guard_duration }}
          FAST_STATUS: ${{ needs.fast-tests.outputs.test_status }}
          FAST_TOTAL: ${{ needs.fast-tests.outputs.total_tests }}
          FAST_PASSED: ${{ needs.fast-tests.outputs.passed_tests }}
          FAST_FAILED: ${{ needs.fast-tests.outputs.failed_tests }}
          FAST_DURATION: ${{ needs.fast-tests.outputs.duration_ms }}
          FAST_PASS_RATE: ${{ needs.fast-tests.outputs.pass_rate }}
          FLAKY_COUNT: ${{ needs.flaky-test-detection.outputs.flaky_count }}
          FLAKY_TESTS: ${{ needs.flaky-test-detection.outputs.flaky_tests }}
          FLAKY_STATUS: ${{ needs.flaky-test-detection.outputs.detection_status }}
          PERF_DURATION: ${{ needs.performance-report.outputs.total_duration_ms }}
          PERF_SETUP: ${{ needs.performance-report.outputs.setup_time_ms }}
          PERF_P95: ${{ needs.performance-report.outputs.p95_ms }}
          PERF_SLOW_COUNT: ${{ needs.performance-report.outputs.slow_test_count }}
        run: |
          set -euo pipefail

          echo "# üìä Test Reliability & Performance Metrics"
          echo ""
          echo "## Test Results"
          echo "| Metric | Value | Status |"
          echo "|--------|-------|--------|"
          echo "| Fast Test Suite | ${FAST_PASSED:-0}/${FAST_TOTAL:-0} passed | ${FAST_STATUS:-unknown} |"
          echo "| Pass Rate | ${FAST_PASS_RATE:-0}% | $([ "${FAST_PASS_RATE:-0}" = "100.00" ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |"
          echo "| Duration | ${FAST_DURATION:-0}ms | ‚è±Ô∏è |"
          echo ""
          echo "## Performance Guard"
          echo "| Metric | Value | Status |"
          echo "|--------|-------|--------|"
          echo "| Guard Status | ${GUARD_STATUS:-unknown} | $([ "${GUARD_STATUS:-}" = "success" ] && echo "‚úÖ" || echo "‚ùå") |"
          echo "| Guard Duration | ${GUARD_DURATION:-0}ms | ‚è±Ô∏è |"
          echo ""
          echo "## Flaky Test Detection"
          echo "| Metric | Value | Status |"
          echo "|--------|-------|--------|"
          echo "| Flaky Tests Found | ${FLAKY_COUNT:-0} | $([ "${FLAKY_COUNT:-0}" = "0" ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |"
          echo "| Detection Status | ${FLAKY_STATUS:-skipped} | ‚ÑπÔ∏è |"
          if [ -n "${FLAKY_TESTS:-}" ] && [ "${FLAKY_TESTS}" != "" ]; then
            echo "| Flaky Test Names | ${FLAKY_TESTS} | ‚ö†Ô∏è |"
          fi
          echo ""
          echo "## Performance Profile"
          echo "| Metric | Value | Target | Status |"
          echo "|--------|-------|--------|--------|"

          DURATION_S=$(awk "BEGIN {printf \"%.1f\", ${PERF_DURATION:-0}/1000}")
          SETUP_S=$(awk "BEGIN {printf \"%.1f\", ${PERF_SETUP:-0}/1000}")

          echo "| Total Duration | ${DURATION_S}s | <60s | $([ "${PERF_DURATION:-0}" -lt "60000" ] && echo "‚úÖ" || echo "‚ùå") |"
          echo "| Setup Time | ${SETUP_S}s | <10s | $([ "${PERF_SETUP:-0}" -lt "10000" ] && echo "‚úÖ" || echo "‚ùå") |"
          echo "| P95 Latency | ${PERF_P95:-0}ms | <200ms | $([ "${PERF_P95:-0}" -lt "200" ] && echo "‚úÖ" || echo "‚ùå") |"
          echo "| Slow Tests | ${PERF_SLOW_COUNT:-0} | <5 | $([ "${PERF_SLOW_COUNT:-0}" -lt "5" ] && echo "‚úÖ" || echo "‚ùå") |"

          # Create JSON metrics file for artifact
          cat > metrics-summary.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "run_id": "${{ github.run_id }}",
            "metrics": {
              "fast_tests": {
                "status": "${FAST_STATUS:-unknown}",
                "total": ${FAST_TOTAL:-0},
                "passed": ${FAST_PASSED:-0},
                "failed": ${FAST_FAILED:-0},
                "duration_ms": ${FAST_DURATION:-0},
                "pass_rate": ${FAST_PASS_RATE:-0}
              },
              "performance_guard": {
                "status": "${GUARD_STATUS:-unknown}",
                "duration_ms": ${GUARD_DURATION:-0}
              },
              "flaky_detection": {
                "status": "${FLAKY_STATUS:-skipped}",
                "flaky_count": ${FLAKY_COUNT:-0},
                "flaky_tests": "${FLAKY_TESTS:-}"
              },
              "performance_profile": {
                "total_duration_ms": ${PERF_DURATION:-0},
                "setup_time_ms": ${PERF_SETUP:-0},
                "p95_ms": ${PERF_P95:-0},
                "slow_test_count": ${PERF_SLOW_COUNT:-0}
              }
            }
          }
          EOF

          echo ""
          echo "üìÅ Metrics saved to metrics-summary.json"

      - name: Upload Metrics Summary
        uses: actions/upload-artifact@65c4c4a1ddee5b72f698fdd19549f0f0fb45cf08 # v4.6.0
        with:
          name: reliability-metrics
          path: metrics-summary.json
          retention-days: 90

  # PR Comment with performance comparison
  pr-performance-comment:
    name: PR Performance Comment
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [fast-tests, performance-guard]
    if: github.event_name == 'pull_request'

    steps:
      - name: Comment Performance Results on PR
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        env:
          GUARD_STATUS: ${{ needs.performance-guard.outputs.guard_status }}
          GUARD_DURATION: ${{ needs.performance-guard.outputs.guard_duration }}
          FAST_STATUS: ${{ needs.fast-tests.outputs.test_status }}
          FAST_TOTAL: ${{ needs.fast-tests.outputs.total_tests }}
          FAST_PASSED: ${{ needs.fast-tests.outputs.passed_tests }}
          FAST_FAILED: ${{ needs.fast-tests.outputs.failed_tests }}
          FAST_DURATION: ${{ needs.fast-tests.outputs.duration_ms }}
          FAST_PASS_RATE: ${{ needs.fast-tests.outputs.pass_rate }}
        with:
          script: |
            const guardStatus = process.env.GUARD_STATUS || 'unknown';
            const guardDuration = parseInt(process.env.GUARD_DURATION || '0');
            const fastStatus = process.env.FAST_STATUS || 'unknown';
            const fastTotal = parseInt(process.env.FAST_TOTAL || '0');
            const fastPassed = parseInt(process.env.FAST_PASSED || '0');
            const fastFailed = parseInt(process.env.FAST_FAILED || '0');
            const fastDuration = parseInt(process.env.FAST_DURATION || '0');
            const fastPassRate = parseFloat(process.env.FAST_PASS_RATE || '0');

            const guardIcon = guardStatus === 'success' ? '‚úÖ' : '‚ùå';
            const testIcon = fastStatus === 'success' ? '‚úÖ' : '‚ùå';
            const rateIcon = fastPassRate >= 99 ? '‚úÖ' : (fastPassRate >= 95 ? '‚ö†Ô∏è' : '‚ùå');

            const durationSec = (fastDuration / 1000).toFixed(1);
            const guardDurationSec = (guardDuration / 1000).toFixed(1);

            const comment = `## üìä Test Performance Report

            | Metric | Value | Status |
            |--------|-------|--------|
            | Fast Tests | ${fastPassed}/${fastTotal} passed | ${testIcon} |
            | Pass Rate | ${fastPassRate.toFixed(2)}% | ${rateIcon} |
            | Duration | ${durationSec}s | ‚è±Ô∏è |
            | Performance Guard | ${guardStatus} | ${guardIcon} |
            | Guard Duration | ${guardDurationSec}s | ‚è±Ô∏è |

            ${fastFailed > 0 ? `\n‚ö†Ô∏è **${fastFailed} test(s) failed** - please review the test output.\n` : ''}

            <details>
            <summary>üìà Performance Thresholds</summary>

            | Threshold | Target | Status |
            |-----------|--------|--------|
            | Fast test duration | <30s | ${fastDuration < 30000 ? '‚úÖ' : '‚ùå'} |
            | Pass rate | ‚â•99% | ${fastPassRate >= 99 ? '‚úÖ' : '‚ùå'} |
            | Guard check | pass | ${guardStatus === 'success' ? '‚úÖ' : '‚ùå'} |

            </details>

            Run \`pnpm test:profile\` locally for detailed performance analysis.
            `;

            // Find existing comment to update
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user?.type === 'Bot' &&
              comment.body?.includes('## üìä Test Performance Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment,
              });
            }
